from detectron2.evaluation import (
    COCOEvaluator
)
from detectron2.utils.logger import create_small_table

from VL_PLM.data.datasets.coco_util import BASE_CATEGORIES, EVAL_CATEGORIES
import numpy as np
from tabulate import tabulate
import itertools


class COCO_evaluator(COCOEvaluator):
    def _derive_coco_results(self, coco_eval, iou_type, class_names=None):

        metrics = {
            "bbox": ["AP", "AP50", "AP75", "APs", "APm", "APl"],
            "segm": ["AP", "AP50", "AP75", "APs", "APm", "APl"],
            "keypoints": ["AP", "AP50", "AP75", "APm", "APl"],
        }[iou_type]

        if coco_eval is None:
            self._logger.warn("No predictions from the model!")
            return {metric: float("nan") for metric in metrics}

        # the standard metrics

        results = {
            metric: float(coco_eval.stats[idx] * 100 if coco_eval.stats[idx] >= 0 else "nan")
            for idx, metric in enumerate(metrics)
        }
        self._logger.info(
            "Evaluation results for {}: \n".format(iou_type) + create_small_table(results)
        )
        if not np.isfinite(sum(results.values())):
            self._logger.info("Some metrics cannot be computed and is shown as NaN.")

        if class_names is None or len(class_names) <= 1:
            return results
        # Compute per-category AP
        # from https://github.com/facebookresearch/Detectron/blob/a6a835f5b8208c45d0dce217ce9bbda915f44df7/detectron/datasets/json_dataset_evaluator.py#L222-L252 # noqa
        precisions = coco_eval.eval["precision"]
        # precision has dims (iou, recall, cls, area range, max dets)
        assert len(class_names) == precisions.shape[2]

        results_per_category = []
        for idx, name in enumerate(class_names):
            # area range index 0: all area ranges
            # max dets index -1: typically 100 per image
            precision = precisions[0, :, idx, 0, -1]  # calculate ap50
            precision = precision[precision > -1]
            ap = np.mean(precision) if precision.size else float("nan")
            results_per_category.append(("{}".format(name), float(ap * 100)))
        # self._logger.info(
        #     "Evaluation results for AP50 \n" + create_small_table({
        #         "results_base": np.mean([i[1] for i in results_per_category if i[0] in BASE_CATEGORIES]),
        #         "results_novel": np.mean([i[1] for i in results_per_category if i[0] in EVAL_CATEGORIES])
        #     })
        # )
        results_base = np.mean([i[1] for i in results_per_category if i[0] in BASE_CATEGORIES])
        results_novel = np.mean([i[1] for i in results_per_category if i[0] in EVAL_CATEGORIES])
        results_all = np.mean([i[1] for i in results_per_category])
        self._logger.info(
            "Evaluation results for AP50 \n" + create_small_table({
                "results_base": results_base,
                "results_novel": results_novel
            })
        )
        if iou_type=="bbox":
            self._logger.info("eval_result AP50@Novel,Base,All:"+" {:.3f}".format(results_novel)+", {:.3f}".format(results_base)+", {:.3f}".format(results_all))

        # tabulate it
        N_COLS = min(6, len(results_per_category) * 2)
        results_flatten = list(itertools.chain(*results_per_category))
        results_2d = itertools.zip_longest(*[results_flatten[i::N_COLS] for i in range(N_COLS)])
        table = tabulate(
            results_2d,
            tablefmt="pipe",
            floatfmt=".3f",
            headers=["category", "AP50"] * (N_COLS // 2),
            numalign="left",
        )
        self._logger.info("Per-category {} AP50: \n".format(iou_type) + table)

        results.update({"AP50-" + name: ap for name, ap in results_per_category})
        return results
